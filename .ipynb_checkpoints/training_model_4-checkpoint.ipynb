{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd7d59-0746-4765-8f56-7a628f457808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mtcnn import MTCNN\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6275421-bea6-4eb9-9fcd-e3525aa53757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "def detect_and_crop_faces(lfw_path, output_path, size=(100, 100)):\n",
    "    detector = MTCNN()\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Iterate over each person folder with tqdm\n",
    "    for person_folder in tqdm(os.listdir(lfw_path), desc=\"Processing persons\", unit=\"person\"):\n",
    "        person_path = os.path.join(lfw_path, person_folder)\n",
    "        if not os.path.isdir(person_path):\n",
    "            continue\n",
    "            \n",
    "        output_person_path = os.path.join(output_path, person_folder)\n",
    "        os.makedirs(output_person_path, exist_ok=True)\n",
    "        \n",
    "        # Iterate over images within each person folder with tqdm\n",
    "        for img_name in tqdm(os.listdir(person_path), desc=f\"Processing {person_folder}\", unit=\"image\"):\n",
    "            img_path = os.path.join(person_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detect faces\n",
    "            faces = detector.detect_faces(img_rgb)\n",
    "            if len(faces) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get the first face\n",
    "            face = faces[0]\n",
    "            x, y, w, h = face['box']\n",
    "            \n",
    "            # Crop and resize\n",
    "            x = max(0, x)\n",
    "            y = max(0, y)\n",
    "            face_img = img_rgb[y:y+h, x:x+w]\n",
    "            face_img = cv2.resize(face_img, size)\n",
    "            \n",
    "            # Save cropped image\n",
    "            output_img_path = os.path.join(output_person_path, img_name)\n",
    "            cv2.imwrite(output_img_path, cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Run face detection\n",
    "lfw_path = \"lfw\"  # Update with your LFW dataset path\n",
    "cropped_path = \"lfw_cropped\"\n",
    "print(\"Detecting and cropping faces...\")\n",
    "detect_and_crop_faces(lfw_path, cropped_path)\n",
    "print(\"Face detection and cropping completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d33861-5059-47ba-8d4a-517f041c339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplet_data(cropped_path, min_images=2):\n",
    "    triplets = []\n",
    "    labels = []\n",
    "    \n",
    "    for person in os.listdir(cropped_path):\n",
    "        person_path = os.path.join(cropped_path, person)\n",
    "        images = [os.path.join(person_path, img) for img in os.listdir(person_path)]\n",
    "        \n",
    "        if len(images) < min_images:\n",
    "            continue\n",
    "            \n",
    "        labels.append(person)\n",
    "        \n",
    "        # Create triplets\n",
    "        for i in range(len(images)):\n",
    "            anchor = images[i]\n",
    "            positive = random.choice([img for img in images if img != anchor])\n",
    "            \n",
    "            # Choose negative from different person\n",
    "            other_labels = [l for l in labels if l != person]\n",
    "            if not other_labels:\n",
    "                continue\n",
    "            negative_person = random.choice(other_labels)\n",
    "            negative_path = os.path.join(cropped_path, negative_person)\n",
    "            negative = random.choice([os.path.join(negative_path, img) \n",
    "                                   for img in os.listdir(negative_path)])\n",
    "            \n",
    "            triplets.append((anchor, positive, negative))\n",
    "    \n",
    "    return triplets\n",
    "\n",
    "# Create triplets\n",
    "print(\"Creating triplet dataset...\")\n",
    "triplets = create_triplet_data(cropped_path)\n",
    "if not triplets:\n",
    "    print(\"No valid triplets found. Check dataset and minimum image requirements.\")\n",
    "else:\n",
    "    print(f\"Created {len(triplets)} triplets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0c3bf-eeff-4ce8-830e-e24bd88cda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return np.zeros((100, 100, 3))  # Return zero array for invalid images\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img / 255.0  # Normalize to [0,1]\n",
    "    return img\n",
    "\n",
    "def triplet_generator(triplets, batch_size=32):\n",
    "    def gen():\n",
    "        while True:\n",
    "            batch_indices = np.random.choice(len(triplets), batch_size)\n",
    "            anchors, positives, negatives = [], [], []\n",
    "            \n",
    "            for idx in batch_indices:\n",
    "                anchor_path, positive_path, negative_path = triplets[idx]\n",
    "                anchors.append(load_and_preprocess_image(anchor_path))\n",
    "                positives.append(load_and_preprocess_image(positive_path))\n",
    "                negatives.append(load_and_preprocess_image(negative_path))\n",
    "            \n",
    "            yield (\n",
    "                (\n",
    "                    np.array(anchors, dtype=np.float32),\n",
    "                    np.array(positives, dtype=np.float32),\n",
    "                    np.array(negatives, dtype=np.float32)\n",
    "                ),\n",
    "                np.zeros(batch_size, dtype=np.float32)\n",
    "            )\n",
    "    \n",
    "    # Define output signature\n",
    "    output_signature = (\n",
    "        (\n",
    "            tf.TensorSpec(shape=(batch_size, 100, 100, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(batch_size, 100, 100, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(batch_size, 100, 100, 3), dtype=tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "    # Create dataset from generator\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "# Split dataset\n",
    "train_triplets, val_triplets = train_test_split(triplets, test_size=0.2, random_state=42)\n",
    "print(f\"Training triplets: {len(train_triplets)}, Validation triplets: {len(val_triplets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a103e1-c6cf-4ad9-8822-a20b5916e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape=(100, 100, 3)):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(128, activation=None)  # Embedding layer\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bf615-80c8-4935-b67a-0e4e52a483b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    anchor, positive, negative = y_pred[:, :128], y_pred[:, 128:256], y_pred[:, 256:]\n",
    "    \n",
    "    # Cosine distance\n",
    "    pos_dist = 1 - tf.reduce_sum(tf.multiply(anchor, positive), axis=-1) / (\n",
    "        tf.sqrt(tf.reduce_sum(tf.square(anchor), axis=-1)) * \n",
    "        tf.sqrt(tf.reduce_sum(tf.square(positive), axis=-1))\n",
    "    )\n",
    "    neg_dist = 1 - tf.reduce_sum(tf.multiply(anchor, negative), axis=-1) / (\n",
    "        tf.sqrt(tf.reduce_sum(tf.square(anchor), axis=-1)) * \n",
    "        tf.sqrt(tf.reduce_sum(tf.square(negative), axis=-1))\n",
    "    )\n",
    "    \n",
    "    loss = tf.maximum(pos_dist - neg_dist + alpha, 0.0)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d848b-f6d8-4ee4-a34e-36545d67a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_model(input_shape=(100, 100, 3)):\n",
    "    base_network = create_base_network(input_shape)\n",
    "    \n",
    "    input_anchor = layers.Input(shape=input_shape)\n",
    "    input_positive = layers.Input(shape=input_shape)\n",
    "    input_negative = layers.Input(shape=input_shape)\n",
    "    \n",
    "    processed_anchor = base_network(input_anchor)\n",
    "    processed_positive = base_network(input_positive)\n",
    "    processed_negative = base_network(input_negative)\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    processed_anchor = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(processed_anchor)\n",
    "    processed_positive = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(processed_positive)\n",
    "    processed_negative = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(processed_negative)\n",
    "    \n",
    "    merged_output = layers.Concatenate(axis=-1)([processed_anchor, processed_positive, processed_negative])\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[input_anchor, input_positive, input_negative],\n",
    "        outputs=merged_output\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=triplet_loss)\n",
    "    return model, base_network\n",
    "\n",
    "# Create model\n",
    "print(\"Creating model...\")\n",
    "model, base_network = create_siamese_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8d0bc-f906-4f33-9036-2acd316a842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "steps_per_epoch = len(train_triplets) // batch_size\n",
    "validation_steps = len(val_triplets) // batch_size\n",
    "\n",
    "print(\"Training model...\")\n",
    "history = model.fit(\n",
    "    triplet_generator(train_triplets, batch_size),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=triplet_generator(val_triplets, batch_size),\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save models\n",
    "model.save('siamese_model.h5')\n",
    "base_network.save('base_network.h5')\n",
    "print(\"Models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7f79a-441a-4cf3-94e2-aad75e312366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9146748-8525-4fd9-adc5-c33079c3b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff535f3d-1d42-4a66-a6f4-8de5fd6347ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base network\n",
    "base_network = load_model('base_network.h5')\n",
    "print(\"Base network loaded successfully.\")\n",
    "\n",
    "# Function to load and preprocess image (same as training)\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return np.zeros((100, 100, 3))  # Return zero array for invalid images\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img / 255.0  # Normalize to [0,1]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf1ae1-7b22-442b-97ce-6c7133b46542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_pairs(cropped_path, num_pairs=500):\n",
    "    pairs = []\n",
    "    labels = []  # 1 for same person, 0 for different person\n",
    "    person_folders = [f for f in os.listdir(cropped_path) if os.path.isdir(os.path.join(cropped_path, f))]\n",
    "    \n",
    "    # Create positive pairs (same person)\n",
    "    for _ in range(num_pairs // 2):\n",
    "        person = random.choice(person_folders)\n",
    "        person_path = os.path.join(cropped_path, person)\n",
    "        images = [os.path.join(person_path, img) for img in os.listdir(person_path)]\n",
    "        if len(images) < 2:\n",
    "            continue\n",
    "        img1, img2 = random.sample(images, 2)\n",
    "        pairs.append((img1, img2))\n",
    "        labels.append(1)\n",
    "    \n",
    "    # Create negative pairs (different people)\n",
    "    for _ in range(num_pairs // 2):\n",
    "        person1, person2 = random.sample(person_folders, 2)\n",
    "        person1_path = os.path.join(cropped_path, person1)\n",
    "        person2_path = os.path.join(cropped_path, person2)\n",
    "        img1 = random.choice([os.path.join(person1_path, img) for img in os.listdir(person1_path)])\n",
    "        img2 = random.choice([os.path.join(person2_path, img) for img in os.listdir(person2_path)])\n",
    "        pairs.append((img1, img2))\n",
    "        labels.append(0)\n",
    "    \n",
    "    return pairs, labels\n",
    "\n",
    "# Generate test pairs\n",
    "cropped_path = \"lfw_cropped\"\n",
    "print(\"Creating test pairs...\")\n",
    "test_pairs, test_labels = create_test_pairs(cropped_path, num_pairs=500)\n",
    "print(f\"Created {len(test_pairs)} test pairs ({sum(test_labels)} positive, {len(test_labels) - sum(test_labels)} negative).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9fd8c6-2d7e-4cb7-8467-94a151ea244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_distance(emb1, emb2):\n",
    "    # Cosine distance = 1 - cosine similarity\n",
    "    dot_product = np.sum(emb1 * emb2, axis=-1)\n",
    "    norm1 = np.sqrt(np.sum(emb1 * emb1, axis=-1))\n",
    "    norm2 = np.sqrt(np.sum(emb2 * emb2, axis=-1))\n",
    "    cosine_similarity = dot_product / (norm1 * norm2 + 1e-10)  # Avoid division by zero\n",
    "    return 1 - cosine_similarity\n",
    "\n",
    "# Compute distances for test pairs\n",
    "distances = []\n",
    "print(\"Computing embeddings and distances...\")\n",
    "for img1_path, img2_path in test_pairs:\n",
    "    # Load and preprocess images\n",
    "    img1 = load_and_preprocess_image(img1_path)\n",
    "    img2 = load_and_preprocess_image(img2_path)\n",
    "    \n",
    "    # Get embeddings\n",
    "    emb1 = base_network.predict(np.expand_dims(img1, axis=0), verbose=0)\n",
    "    emb2 = base_network.predict(np.expand_dims(img2, axis=0), verbose=0)\n",
    "    \n",
    "    # Compute cosine distance\n",
    "    distance = compute_cosine_distance(emb1, emb2)\n",
    "    distances.append(distance)\n",
    "\n",
    "distances = np.array(distances)\n",
    "print(\"Distance computation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1e6aa-10a5-42ae-b9cc-0486f917b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(distances, labels, threshold=0.5):\n",
    "    # Predict same person if distance < threshold\n",
    "    predictions = (distances < threshold).astype(int)\n",
    "    return accuracy_score(labels, predictions)\n",
    "\n",
    "# Evaluate with a default threshold\n",
    "threshold = 0.5\n",
    "accuracy = evaluate_accuracy(distances, test_labels, threshold)\n",
    "print(f\"Accuracy with threshold {threshold}: {accuracy:.4f}\")\n",
    "\n",
    "# Try multiple thresholds to find the best\n",
    "thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "accuracies = [evaluate_accuracy(distances, test_labels, t) for t in thresholds]\n",
    "best_threshold = thresholds[np.argmax(accuracies)]\n",
    "best_accuracy = max(accuracies)\n",
    "print(f\"Best threshold: {best_threshold:.2f}, Best accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a554180-1f2f-4bd8-a839-a3b49c567eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate distances by label\n",
    "positive_distances = distances[np.array(test_labels) == 1]\n",
    "negative_distances = distances[np.array(test_labels) == 0]\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(positive_distances, bins=30, alpha=0.5, label='Positive Pairs (Same Person)', color='blue')\n",
    "plt.hist(negative_distances, bins=30, alpha=0.5, label='Negative Pairs (Different Person)', color='red')\n",
    "plt.axvline(x=best_threshold, color='green', linestyle='--', label=f'Best Threshold ({best_threshold:.2f})')\n",
    "plt.title('Distribution of Cosine Distances')\n",
    "plt.xlabel('Cosine Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e5abc-56bc-404c-9741-07af0a0a3a64",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c76c28-47d7-411b-b580-1caaa365f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from mtcnn import MTCNN\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the base network\n",
    "base_network = load_model('base_network.h5')\n",
    "print(\"Base network loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d8721-0f55-430c-8ff4-834e26230b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return np.zeros((100, 100, 3))  # Return zero array for invalid images\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img / 255.0  # Normalize to [0,1]\n",
    "    return img\n",
    "\n",
    "# Compute cosine distance\n",
    "def compute_cosine_distance(emb1, emb2):\n",
    "    dot_product = np.sum(emb1 * emb2, axis=-1)\n",
    "    norm1 = np.sqrt(np.sum(emb1 * emb1, axis=-1))\n",
    "    norm2 = np.sqrt(np.sum(emb2 * emb2, axis=-1))\n",
    "    cosine_similarity = dot_product / (norm1 * norm2 + 1e-10)\n",
    "    return (1 - cosine_similarity).item()\n",
    "\n",
    "# Detect and crop face\n",
    "def detect_and_crop_face(image, size=(100, 100)):\n",
    "    detector = MTCNN()\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    faces = detector.detect_faces(img_rgb)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    face = faces[0]\n",
    "    x, y, w, h = face['box']\n",
    "    x, y = max(0, x), max(0, y)\n",
    "    face_img = img_rgb[y:y+h, x:x+w]\n",
    "    face_img = cv2.resize(face_img, size)\n",
    "    return face_img / 255.0  # Normalize to [0,1]\n",
    "\n",
    "# Log attendance\n",
    "def log_attendance(person, csv_path=\"attendance.csv\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    entry = {\"Timestamp\": timestamp, \"Person\": person}\n",
    "    df = pd.DataFrame([entry])\n",
    "    if os.path.exists(csv_path):\n",
    "        df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, mode='w', header=True, index=False)\n",
    "    print(f\"Attendance logged for {person} at {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b115eeb-0220-48bd-a082-44009e5d9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_images(person_name, output_dir=\"new_person_images\", num_images=5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Capturing {num_images} images for {person_name}. Press 'c' to capture, 'q' to quit.\")\n",
    "    captured = 0\n",
    "    \n",
    "    while captured < num_images:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "        \n",
    "        cv2.imshow(\"Capture Image\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        if key == ord('c'):\n",
    "            img_path = os.path.join(output_dir, f\"{person_name}_{captured+1}.jpg\")\n",
    "            cv2.imwrite(img_path, frame)\n",
    "            print(f\"Saved image: {img_path}\")\n",
    "            captured += 1\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Capture images for the new person\n",
    "new_person_name = \"amin\"  # Replace with the new person's name\n",
    "capture_images(new_person_name, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222c054-8670-4a78-81ae-7de4847926c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_faces(input_dir, person_name, output_dir=\"face_database\", size=(100, 100)):\n",
    "    detector = MTCNN()\n",
    "    person_output_path = os.path.join(output_dir, person_name)\n",
    "    os.makedirs(person_output_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing images for {person_name}...\")\n",
    "    for img_name in os.listdir(input_dir):\n",
    "        img_path = os.path.join(input_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = detector.detect_faces(img_rgb)\n",
    "        if len(faces) == 0:\n",
    "            print(f\"No face detected in {img_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Crop and resize\n",
    "        face = faces[0]\n",
    "        x, y, w, h = face['box']\n",
    "        x, y = max(0, x), max(0, y)\n",
    "        face_img = img_rgb[y:y+h, x:x+w]\n",
    "        face_img = cv2.resize(face_img, size)\n",
    "        \n",
    "        # Save cropped image\n",
    "        output_img_path = os.path.join(person_output_path, img_name)\n",
    "        cv2.imwrite(output_img_path, cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"Saved cropped image: {output_img_path}\")\n",
    "\n",
    "# Crop images for the new person\n",
    "input_dir = \"new_person_images\"\n",
    "face_database_path = \"face_database\"\n",
    "crop_and_save_faces(input_dir, new_person_name, face_database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912bf53-807f-4815-bcda-2b1169de818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_database(face_db_path, max_images_per_person=5):\n",
    "    database = {}\n",
    "    print(\"Building embedding database...\")\n",
    "    \n",
    "    for person in os.listdir(face_db_path):\n",
    "        person_path = os.path.join(face_db_path, person)\n",
    "        if not os.path.isdir(person_path):\n",
    "            continue\n",
    "        images = [os.path.join(person_path, img) for img in os.listdir(person_path)]\n",
    "        if not images:\n",
    "            continue\n",
    "        images = random.sample(images, min(len(images), max_images_per_person))\n",
    "        embeddings = []\n",
    "        for img_path in images:\n",
    "            img = load_and_preprocess_image(img_path)\n",
    "            emb = base_network.predict(np.expand_dims(img, axis=0), verbose=0)\n",
    "            embeddings.append(emb[0])\n",
    "        database[person] = np.array(embeddings)\n",
    "        print(f\"Added {person} with {len(embeddings)} embeddings\")\n",
    "    \n",
    "    return database\n",
    "\n",
    "def add_person_to_database(person_name, face_db_path, database, max_images=5):\n",
    "    person_path = os.path.join(face_db_path, person_name)\n",
    "    if not os.path.exists(person_path):\n",
    "        print(f\"Error: No images found for {person_name} in {face_db_path}\")\n",
    "        return database\n",
    "    \n",
    "    images = [os.path.join(person_path, img) for img in os.listdir(person_path)]\n",
    "    if not images:\n",
    "        print(f\"No valid images for {person_name}\")\n",
    "        return database\n",
    "    \n",
    "    images = random.sample(images, min(len(images), max_images))\n",
    "    embeddings = []\n",
    "    for img_path in images:\n",
    "        img = load_and_preprocess_image(img_path)\n",
    "        emb = base_network.predict(np.expand_dims(img, axis=0), verbose=0)\n",
    "        embeddings.append(emb[0])\n",
    "    database[person_name] = np.array(embeddings)\n",
    "    print(f\"Added {person_name} with {len(embeddings)} embeddings\")\n",
    "    \n",
    "    return database\n",
    "\n",
    "# Build or update database\n",
    "face_database_path = \"face_database\"\n",
    "if 'embedding_database' not in globals():\n",
    "    embedding_database = build_embedding_database(face_database_path)\n",
    "else:\n",
    "    embedding_database = add_person_to_database(new_person_name, face_database_path, embedding_database)\n",
    "print(f\"Database contains {len(embedding_database)} individuals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61be388-20a2-435f-a963-3ca9cfbf5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_person(image, database, threshold=0.5):\n",
    "    face_img = detect_and_crop_face(image)\n",
    "    if face_img is None:\n",
    "        return None, None\n",
    "    \n",
    "    emb = base_network.predict(np.expand_dims(face_img, axis=0), verbose=0)[0]\n",
    "    \n",
    "    min_distance = float('inf')\n",
    "    identified_person = None\n",
    "    \n",
    "    for person, embeddings in database.items():\n",
    "        for db_emb in embeddings:\n",
    "            distance = compute_cosine_distance(emb, db_emb)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                identified_person = person\n",
    "    \n",
    "    if min_distance > threshold:\n",
    "        return None, min_distance\n",
    "    return identified_person, min_distance\n",
    "\n",
    "# Test with a new image\n",
    "test_img_path = os.path.join(\"new_person_images\", f\"{new_person_name}_1.jpg\")\n",
    "if os.path.exists(test_img_path):\n",
    "    test_img = cv2.imread(test_img_path)\n",
    "    person, distance = identify_person(test_img, embedding_database, threshold=0.5)\n",
    "    print(f\"Test image identified as: {person}, Distance: {distance:.4f}\")\n",
    "    if person == new_person_name:\n",
    "        log_attendance(person)\n",
    "    else:\n",
    "        print(\"Identification failed. Attendance not logged.\")\n",
    "else:\n",
    "    print(f\"Test image not found: {test_img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05376481-f934-44ad-b3c7-d7d48cf1e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_attendance(database, threshold=0.5, csv_path=\"attendance.csv\"):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Press 'q' to quit webcam.\")\n",
    "    last_logged = {}\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "        \n",
    "        person, distance = identify_person(frame, database, threshold)\n",
    "        \n",
    "        if person:\n",
    "            text = f\"{person} (Dist: {distance:.2f})\"\n",
    "            cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            current_time = datetime.now()\n",
    "            if person not in last_logged or (current_time - last_logged[person]).total_seconds() > 60:\n",
    "                log_attendance(person, csv_path)\n",
    "                last_logged[person] = current_time\n",
    "        else:\n",
    "            cv2.putText(frame, \"Unknown\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        cv2.imshow(\"Attendance System\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test real-time recognition\n",
    "real_time_attendance(embedding_database, threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
